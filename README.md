# BERT

<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->

- [Fundamentals](#fundamentals)
- [Links](#links)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

## Fundamentals

- possibility for transfer learning and benefit from pre-trained model
- BERT is made out of transformers without recurrence (RNNs)
- BERT has its own tokenizer and a fixed vocabulary with 30k tokens
- breaks down unknown words into subword tokens
- one BERT token (word embedding) has 768 features

## Links

- [The Annotated Transformer](http://nlp.seas.harvard.edu/2018/04/03/attention.html) by Rush et. Al.
- [The Illustrated Transformer](http://jalammar.github.io/illustrated-transformer/) by Jay Allamar
- [Official Repository](https://github.com/google-research/bert) by Google
- [BERT Paper](https://arxiv.org/pdf/1810.04805.pdf) by Devlin et. Al.
